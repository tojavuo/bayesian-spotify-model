---
title: "bayesian-spotify-analysis"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Using songâ€™s features to predict its popularity

#Introduction

Is there a recipe for a hit song? Would it be possible to construct an artificial intelligence system that predicts how popular a specific song is going to be? This is something we set out to explore in this project.

To delve deeper into this topic, we set out to build Bayesian models that could help explain what makes a song popular. Well suited for this, Spotify automatically generates a popularity measure for each of their songs so we decided to use them for training our model.

#The data

Our original plan was to use the API offered by Spotify to fetch songs and their attributes, but it turned out that there already was a large dataset that contained all the relevant song information and various different songs. This data was obtained from Kaggle [1] and consisted of 20 columns and over 586 000 rows. The variables in the data included attributes such as song duration (in ms), danceability (between 0 and 1), energy (between 0 and 1) and popularity (between 0 and 100). These attributes are often generated by Spotify automatically and the way they are exactly evaluated is not available for public. In the case of popularity we do know that it is affected by both the amounts of listens, as well as how recent those listens were. Out of the 20 attributes, we considered song duration, danceability, energy and instrumentalness as our explanatory variables for the popularity of the song.

#The model

we constructed a pooled model with multiple predictors. As a response variable we used the popularity $P_i$ of track $i$, and as explanatory variables we initially used the danceability $D_i$, energy $E_i$, instrumentalness $I_i$ and duration $Du_i$, of track $i$. The model can be written as:
$$
    P_i = \alpha + \beta D_i + \gamma E_i + \delta Du_i + \varepsilon I_i + \epsilon_i, 
$$
where $\epsilon_i \sim \textrm{N}(0,\sigma)$. This is equivalent to:
$$
    P_i \sim \textrm{N}(\alpha + \beta D_i + \gamma E_i + \delta Du_i + \varepsilon I_i, \sigma).
$$
We also group the data by musical mode to see if the parameters for major-key tracks differ from the parameters for minor-key songs.

#Model priors

When considering priors, we used Ashrith Shetty's analysis of Spotify's top tracks of 2017, titled "What Makes a Song Likeable?" [2], as a guideline for our prior choice. Based on Shetty's visualizations, the most popular tracks are often danceable and energetic. Thus one would expect the coefficients $\beta$ and $\gamma$, corresponding to explanatory variables danceability and energy, to be positive. 

On the other hand, most of Spotify's top tracks have an instrumentalness score of close to 0: popular tracks tend to include vocals. In addition, popular tracks tend to be rather short. The average song length on the Billboard Hot 100 chart is 3 minutes and 30 seconds [3]. Thus one would expect the coefficients $\delta$ and $\varepsilon$ to be negative.

By itself, Shetty's visualization is not a very convincing (nor quantitative) piece of information. Thus, we decided to use weakly informative normal priors for the models, with means that are quite close to 0 and chosen to be positive for $\beta$ and $\gamma$, and negative for $\delta$ and $\varepsilon$. For $\alpha$ we used a Cauchy(0,40) prior, since most songs have a small but nonzero popularity score. The priors used for the other parameters were: $\beta, \gamma \sim $ N(3,20) and $\delta, \varepsilon \sim$ N(-3,20).

The popularity score given by Spotify is some (possibly weighted) sum of recent listens. If, for the sake of simplicity, we assume the listens to be independent, we can use the central limit theorem to justify the use of a Gaussian model in the analysis. The theorem states that the distribution of a sum of independent random variables tends to the Gaussian distribution, regardless of the distributions of the independent random variables. The underlying assumption of this analysis is thus that for all songs with some fixed values of the explanatory variables $D$, $E$, $Du$ and $I$, the popularity scores of those songs would follow the normal distribution.

#The implementation

The model was written in Stan, a probabilistic programming language for statistical inference. The Stan model was then run using R.

At first, the model was run using the four predictors $D$, $E$, $Du$ and $I$. However, the results and concergence were not satisfactory; the coefficient $\delta$ of the predictor $Du$ was estimated to be almost surely zero and $Du$ was thus discarded from the analysis. This solved the problem. We discuss convergence more later on in the report.

The model in Stan language:

```{stan output.var=}
data {
  int<lower=0> N;                  // number of data items
  vector<lower=0, upper=1>[N] D;   // observed danceabilities
  vector<lower=0, upper=1>[N] E;   // observed energies
  vector<lower=0>[N] Du;           // observed durations
  vector<lower=0, upper=1>[N] I;   // observed instrumentalnesses
  vector<lower=0, upper=100>[N] y; // popularity vector
  
  int<lower=0> N_test; // number of data items in test set
  vector<lower=0, upper=1>[N_test] D_test;   // observed danceabilities
  vector<lower=0, upper=1>[N_test] E_test;   // observed energies
  vector<lower=0>[N_test] Du_test;           // observed durations
  vector<lower=0, upper=1>[N_test] I_test;   // observed instrumentalnesses
  vector<lower=0, upper=100>[N_test] y_test; // popularity vector
  
}
parameters {
  real<lower=0> alpha; // intercept
  real beta;           // coefficient 1 for predictor D
  real gamma;          // coefficient 2 for predictor E
  //real delta;        //coefficient 3 for predictor Du
  real epsilon;        //coefficient 4 for predictor I
  real<lower=0> sigma; // error scale
}

model {
  y ~ normal(alpha + D * beta + E * gamma + I * epsilon, sigma); // likelihood
  alpha ~ cauchy(0,40);    // prior
  beta ~ normal(3,20);     // prior
  gamma ~ normal(3,20);    // prior
  //delta ~ normal(-3,20); //prior
  epsilon ~ normal(-3,20); //prior
  sigma ~ normal(0,50);    //prior
}

generated quantities { 
  vector[N] y_new; //predicted values for training set
  vector[N_test] y_new_test; //predicted values for test set
  vector[N] log_lik; //log likelihoods to obtain Pareto K values
  
  for (n in 1:N) {
    y_new[n] = fmax(normal_rng(alpha + D[n] * beta + E[n] * gamma + I[n] * epsilon, sigma), 0);
  }
  
  for (i in 1:N_test) {
    y_new_test[i] = fmax(normal_rng(alpha + D_test[i] * beta + E_test[i] * gamma + I_test[i] * epsilon, sigma), 0);
  }
  
  for (n in 1:N) {
    log_lik[n] = normal_lpdf( y[n] | alpha + D[n] * beta + E[n] * gamma + I[n] * epsilon, sigma);
  }
}

```

Now for the R implementation, the following packages were used:

```{r message=FALSE}
library(rstan)
library(ggplot2)
library(rstudioapi)
library(dplyr)
library(MLmetrics)
library(loo)
library(bayesplot)
```

We store samples of the data from Kaggle:

```{r}
data <- read.csv("./tracks.csv")

act_data = data[sample(nrow(data), 10000), ]
# split songs, get relevant parameters
major_songs = act_data %>% filter(mode == 1)
major_songs = major_songs[sample(nrow(major_songs), 2000), ] ##random sample of 2000 observations to keep computation times feasible
minor_songs = act_data %>% filter(mode == 0)
minor_songs = minor_songs[sample(nrow(minor_songs), 2000), ] ##random sample of 2000 observations to keep computation times feasible

minor_pop = minor_songs$popularity
minor_danc = minor_songs$danceability
minor_ener = minor_songs$energy
minor_dur = minor_songs$duration_ms
minor_ins = minor_songs$instrumentalness

major_pop = major_songs$popularity
major_danc = major_songs$danceability
major_ener = major_songs$energy
major_dur = major_songs$duration_ms
major_ins = major_songs$instrumentalness

len_major = nrow(major_songs)
len_minor = nrow(minor_songs)

testdata = data[sample(nrow(data), 5000), ] ##random samples of 1000 observations were used for predictive performance assessment

major_testdata = testdata %>% filter(mode == 1)
major_testdata = major_testdata[sample(nrow(major_testdata), 1000), ]
len_majtest = nrow(major_testdata)

test_major_pop = major_testdata$popularity
test_major_danc = major_testdata$danceability
test_major_ener = major_testdata$energy
test_major_dur = major_testdata$duration_ms
test_major_ins = major_testdata$instrumentalness

minor_testdata = testdata %>% filter(mode == 0)
minor_testdata = minor_testdata[sample(nrow(minor_testdata), 1000), ]
len_mintest = nrow(minor_testdata)

test_minor_pop = minor_testdata$popularity
test_minor_danc = minor_testdata$danceability
test_minor_ener = minor_testdata$energy
test_minor_dur = minor_testdata$duration_ms
test_minor_ins = minor_testdata$instrumentalness
```

Stan requires the data to be in a list format:

```{r}
stan_data_major <- list(
  N = len_major,
  D = major_danc,
  E = major_ener,
  Du = major_dur,
  I = major_ins,
  y = major_pop,
  N_test = len_majtest,
  D_test = test_major_danc,
  E_test = test_major_ener,
  Du_test = test_major_dur,
  I_test = test_major_ins,
  y_test = test_major_pop
)

stan_data_minor <- list(
  N = len_minor,
  D = minor_danc,
  E = minor_ener,
  Du = minor_dur,
  I = minor_ins,
  y = minor_pop,
  N_test = len_mintest,
  D_test = test_minor_danc,
  E_test = test_minor_ener,
  Du_test = test_minor_dur,
  I_test = test_minor_ins,
  y_test = test_minor_pop
)
```

Now, we fit the models for both major- and minor-key songs.

```{r}
major_fit = stan(file="pred_model.stan",
           data=stan_data_major,
           chains=4,
           cores=8)
monitor(major_fit)

minor_fit = stan(file="pred_model.stan",
           data=stan_data_minor,
           chains=4,
           cores=8)
monitor(minor_fit)
```
There are the results for the parameters. We notice some differences between the major-key and minor-key models. We also notice that nearly all of the 95% credible intervals of the parameters do not contain zero. We also did sensitivity analysis with narrower priors whose means were changed to negative or positive. This changed the results very little and thus the model is not very sensitive to prior choice:

```{r}
alt_major_fit = stan(file="alt_spotify_model.stan",
           data=stan_data_major,
           chains=4,
           cores=8)
monitor(alt_major_fit)

alt_minor_fit = stan(file="alt_spotify_model.stan",
           data=stan_data_minor,
           chains=4,
           cores=8)
monitor(alt_minor_fit)
```
Here are the results of the inference neatly visualized for major-key songs (minor-key results were similar):

```{r}
mcmc_areas(as.matrix(major_fit),
           pars = c("alpha", "beta", "gamma", "epsilon", "sigma"),
           prob = 0.95) + ggtitle("Posterior distributions", "with medians and 95% intervals, major key")
mcmc_areas(as.matrix(alt_major_fit),
           pars = c("alpha", "beta", "gamma", "epsilon", "sigma"),
           prob = 0.95) + ggtitle("Posterior distributions using alternative priors", "with medians and 95% intervals, major key")
```

We extract the data to do further analysis:

```{r}
ext_minor = extract(minor_fit)
ext_major = extract(major_fit)
```

#Predictive performance assessment

To assess predictive performance, we calculate the root mean square errors for both models: 

```{r}
data_pred_maj = apply(ext_major$y_new_test, 2, median)
sqrt(mean((major_testdata$popularity - data_pred_maj)^2)) #root mean square error for major key

data_pred_min = apply(ext_minor$y_new_test, 2, median)
sqrt(mean((minor_testdata$popularity - data_pred_min)^2)) #root mean square error for minor key
```

The prediction is significantly better than predicting 0 (most common popularity value) for all songs:

```{r}
rep_major = rep(0, length(ext_major$y_new_test))
rep_minor = rep(0, length(ext_minor$y_new_test))

sqrt(mean((major_testdata$popularity - rep_major)^2))
sqrt(mean((minor_testdata$popularity - rep_minor)^2))
```

Visual posterior predictive check:

```{r}
ppc_dens_overlay(y = test_major_pop,
                 yrep = ext_major$y_new_test) + ggtitle("Observations and posterior predictive distributions, major key") + xlab("Popularity") + bayesplot:::scale_color_ppc_dist(labels = c("y", "y_tilde")) + 
  bayesplot:::scale_fill_ppc_dist(labels = c("y", "y_tilde"))
```

```{r}
ppc_dens_overlay(y = test_minor_pop,
                 yrep = ext_minor$y_new_test) + ggtitle("Observations and posterior predictive distributions, major key") + xlab("Popularity") + bayesplot:::scale_color_ppc_dist(labels = c("y", "y_tilde")) + bayesplot:::scale_fill_ppc_dist(labels = c("y", "y_tilde"))
```

The predicted distributions look very similar to the observed distributions. 

#Model comparison

For comparison, we also fitted a hierarchical model. The difference to the original model is that coefficients $\beta$ and $\gamma$ are thought to come from the same distribution, here the normal distribution with some parameters $\mu$ and $\sigma$.

```{r}
hier_major_fit = stan(file="hier_spotify_model.stan",
                      data=stan_data_major,
                      chains=4,
                      cores=8)

hier_minor_fit = stan(file="hier_spotify_model.stan",
                      data=stan_data_minor,
                      chains=4,
                      cores=8)
```
However, from the LOO comparison we notice that the hierarchical model does not perfrom any better than the pooled model. This is true for both the major-key and the minor-key models. Thus we discarded the hierarchical model, as it is more complex:

```{r}
log_lik1 = extract_log_lik(major_fit)
log_lik2 = extract_log_lik(hier_major_fit)
major_loo = loo(log_lik1)
hier_loo = loo(log_lik2)
plot(major_loo)
plot(hier_loo)
loo_compare(major_loo, hier_loo)
```
All of the pareto K values are relatively small, indicating that none of the observations are highly influential. Thus the LOO estimates can be considered reliable.
